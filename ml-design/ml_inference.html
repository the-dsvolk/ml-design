<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive ML Inference System Design</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Chosen Palette: Warm Neutral with Teal Accents */
        /* Application Structure Plan: Single-page application with a fixed top navigation bar. Content is divided into thematic sections (Introduction, Core Components, Industry Insights, Best Practices, System Architecture, Future Trends). Core Components will use internal tabs for sub-sections. This structure provides a clear, hierarchical way to explore the dense information from the report, allowing users to either read sequentially or jump to topics of interest. Interactivity includes navigation-driven content display and accordions for detailed best practices. */
        /* Visualization & Content Choices: Report content is primarily textual and conceptual. Key information (summaries, lists, tables) will be extracted. The existing Mermaid diagram will be embedded. Tables from the report (Serving Runtimes, Caching Strategies, JUDE, DeepSeek-V3 features) will be rendered as responsive HTML tables. No new charts are created as the report lacks suitable quantitative data for Chart.js visualizations. The goal is to make the rich textual and diagrammatic information accessible and navigable. Interactions focus on content organization (tabs, accordions) and navigation. CONFIRMATION: NO SVG graphics used. NO Mermaid JS used (except for rendering the existing diagram from the source report). */
        body {
            font-family: 'Inter', sans-serif;
        }
        .nav-link {
            @apply px-3 py-2 rounded-md text-sm font-medium text-stone-700 hover:bg-teal-500 hover:text-white transition-colors;
        }
        .nav-link.active {
            @apply bg-teal-600 text-white;
        }
        .content-section {
            @apply hidden p-4 md:p-6;
        }
        .content-section.active {
            @apply block;
        }
        .tab-button {
            padding: 0.5rem 1rem;
            font-weight: 500;
            font-size: 0.875rem;
            color: #57534e;
            background-color: transparent;
            border: none;
            border-top-left-radius: 0.5rem;
            border-top-right-radius: 0.5rem;
            transition: all 0.2s ease;
            cursor: pointer;
        }
        .tab-button:hover {
            background-color: #f0fdfa;
            color: #0f766e;
        }
        .tab-button.active {
            background-color: #0d9488;
            color: white;
        }
        .tab-button.active:hover {
            background-color: #0f766e;
        }
        .tab-content {
            display: none;
            padding: 1rem;
            border: 1px solid #d6d3d1;
            border-top: none;
            border-bottom-left-radius: 0.5rem;
            border-bottom-right-radius: 0.5rem;
            background-color: white;
        }
        .tab-content.active {
            display: block;
        }
        .accordion-button {
            @apply w-full text-left px-4 py-3 bg-stone-200 hover:bg-stone-300 text-stone-800 font-semibold rounded-lg focus:outline-none transition-colors flex justify-between items-center;
        }
        .accordion-content {
            @apply hidden p-4 mt-2 border border-stone-300 rounded-b-lg bg-white;
        }
        .table-responsive {
            @apply overflow-x-auto;
        }
        .table {
            @apply min-w-full divide-y divide-stone-200;
        }
        .table th {
            @apply px-6 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider bg-stone-50;
        }
        .table td {
            @apply px-6 py-4 whitespace-normal text-sm text-stone-700;
        }
        .table tbody tr:nth-child(even) {
            @apply bg-stone-50;
        }
        h2 { 
            font-size: 1.5rem;
            font-weight: 700;
            color: #0f766e;
            margin-bottom: 1rem;
            margin-top: 1.5rem;
            border-bottom: 2px solid #0d9488;
            padding-bottom: 0.5rem;
        }
        h3 { 
            font-size: 1.25rem;
            font-weight: 600;
            color: #0d9488;
            margin-bottom: 0.75rem;
            margin-top: 1rem;
        }
        h4 { 
            font-size: 1.125rem;
            font-weight: 600;
            color: #44403c;
            margin-bottom: 0.5rem;
            margin-top: 0.75rem;
        }
        p { 
            margin-bottom: 0.75rem;
            color: #44403c;
            line-height: 1.6;
        }
        ul { 
            list-style-type: disc;
            list-style-position: inside;
            margin-bottom: 0.75rem;
            padding-left: 1rem;
            color: #44403c;
        }
        strong { 
            font-weight: 600;
            color: #292524;
        }

        /* Chart Container Styling (for Mermaid diagram and potential future charts) */
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 1200px; /* Increased width for more detailed diagram */
            margin-left: auto;
            margin-right: auto;
            @apply p-4 bg-white rounded-lg shadow;
        }
        .mermaid {
            @apply w-full h-auto text-xs; /* Smaller text for denser diagram */
            /* overflow: auto; */ /* Add scroll for very large diagrams if needed */
        }

    </style>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ 
            startOnLoad: true, 
            theme: 'base', 
            themeVariables: {
                'primaryColor': '#F0F9FF', // Light blue for primary nodes
                'primaryTextColor': '#0C4A6E', // Dark blue text
                'primaryBorderColor': '#7DD3FC', // Sky blue border
                'lineColor': '#334155', // Slate-700 for lines
                'textColor': '#1f2937', // Gray-800 for general text
                'fontSize': '12px', // Smaller font for denser diagram
                'secondaryColor': '#FFFBEB', // Light yellow for secondary
                'tertiaryColor': '#F0FDFA', // Light teal for tertiary
                'clusterBkg': '#FAFAFA', // Lighter background for subgraphs
                'clusterBorder': '#A1A1AA' // Neutral border for subgraphs
            },
            flowchart: {
                htmlLabels: true, // Allow HTML in labels for better formatting if needed
                useMaxWidth: false // Let nodes expand as needed
            }
        });
    </script>
</head>
<body class="bg-amber-50 text-stone-800">

    <header class="bg-white shadow-md sticky top-0 z-50">
        <nav class="container mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex items-center justify-between h-16">
                <div class="flex items-center">
                    <span class="font-bold text-xl text-teal-700">ML Inference Systems</span>
                </div>
                <div class="hidden md:block">
                    <div class="ml-10 flex items-baseline space-x-4">
                        <a href="#introduction" class="nav-link active">Introduction</a>
                        <a href="#core-components" class="nav-link">Core Components</a>
                        <a href="#industry-insights" class="nav-link">Industry Insights</a>
                        <a href="#best-practices" class="nav-link">Best Practices</a>
                        <a href="#system-architecture" class="nav-link">System Architecture</a>
                        <a href="#future-trends" class="nav-link">Future Trends</a>
                    </div>
                </div>
                <div class="md:hidden">
                    <button id="mobile-menu-button" class="inline-flex items-center justify-center p-2 rounded-md text-stone-400 hover:text-white hover:bg-teal-600 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-white">
                        <span class="sr-only">Open main menu</span>
                        <svg class="block h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" aria-hidden="true">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />
                        </svg>
                        <svg class="hidden h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" aria-hidden="true">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                        </svg>
                    </button>
                </div>
            </div>
        </nav>
        <div id="mobile-menu" class="md:hidden hidden">
            <div class="px-2 pt-2 pb-3 space-y-1 sm:px-3">
                <a href="#introduction" class="nav-link block">Introduction</a>
                <a href="#core-components" class="nav-link block">Core Components</a>
                <a href="#industry-insights" class="nav-link block">Industry Insights</a>
                <a href="#best-practices" class="nav-link block">Best Practices</a>
                <a href="#system-architecture" class="nav-link block">System Architecture</a>
                <a href="#future-trends" class="nav-link block">Future Trends</a>
            </div>
        </div>
    </header>

    <main class="container mx-auto px-4 sm:px-6 lg:px-8 py-8">
        <section id="introduction" class="content-section active">
            <h2>I. Introduction to High-Performance ML Inference Systems</h2>
            <p>The proliferation of artificial intelligence (AI) across industries has created an escalating demand for applications capable of leveraging machine learning (ML) models to deliver intelligent features. Central to these applications is the ML inference system, responsible for executing trained models on new, live data to generate predictions. The design and implementation of these inference systems are critical, as they must often handle fluctuating workloads, deliver predictions with minimal latency, and maintain high levels of availability and reliability.</p>
            <p>The ML lifecycle encompasses two primary phases: model training and model inference. This application focuses on the latter, detailing the architecture of modern ML inference systems.</p>
            <h3>Overview of Key Design Considerations</h3>
            <p>Designing an effective ML inference system requires careful consideration of several interconnected factors:</p>
            <ul>
                <li><strong>Scalability:</strong> Handle variations in request volume.</li>
                <li><strong>Latency:</strong> Minimize prediction generation time.</li>
                <li><strong>Throughput:</strong> Process a high number of requests per unit of time.</li>
                <li><strong>Cost-Effectiveness:</strong> Optimize resource utilization.</li>
                <li><strong>Maintainability:</strong> Facilitate easy updates and management.</li>
                <li><strong>Observability:</strong> Monitor system health, performance, and model behavior.</li>
                <li><strong>Security:</strong> Protect model IP, data, and system integrity.</li>
            </ul>
            <p>The choices made for each component have cascading effects. For instance, sophisticated runtimes like NVIDIA Triton influence load balancer and API gateway configurations, while large MoE models like DeepSeek-V3 demand specialized infrastructure. This underscores the need for a holistic design approach.</p>
        </section>

        <section id="core-components" class="content-section">
            <h2>II. Core Architectural Components for ML Inference</h2>
            <p>A modern ML inference system comprises several key components, each playing a distinct role. Explore the components using the tabs below.</p>
            <div class="my-4">
                <div class="border-b border-stone-300">
                    <nav id="core-components-tabs" class="-mb-px flex space-x-4" aria-label="Tabs">
                        <button data-target="client-interaction" class="tab-button active">Client Interaction</button>
                        <button data-target="api-gateway" class="tab-button">API Gateway</button>
                        <button data-target="load-balancing" class="tab-button">Load Balancing</button>
                        <button data-target="rate-limiting" class="tab-button">Rate Limiting</button>
                        <button data-target="model-serving-cluster" class="tab-button">Serving Cluster</button>
                        <button data-target="caching-layers" class="tab-button">Caching</button>
                        <button data-target="model-artifacts" class="tab-button">Model Artifacts</button>
                        <button data-target="feature-stores" class="tab-button">Feature Stores</button>
                        <button data-target="monitoring" class="tab-button">Monitoring</button>
                    </nav>
                </div>
                <div id="client-interaction-content" class="tab-content active">
                    <h3>A. Client Interaction Patterns</h3>
                    <p>The way clients interact with the ML inference API significantly shapes the system architecture. Common patterns include:</p>
                    <ul>
                        <li><strong>Synchronous (Real-time):</strong> Client blocks awaiting an immediate response. Critical for low-latency applications (e.g., recommendations, fraud detection).</li>
                        <li><strong>Asynchronous (Near Real-time):</strong> Client gets immediate acknowledgment; processing happens in the background. Suited for longer tasks or larger payloads.</li>
                        <li><strong>Batch (Offline):</strong> Processing large datasets where latency isn't primary. Predictions stored for later use.</li>
                        <li><strong>Serverless Inference:</strong> Ideal for intermittent traffic, scaling to zero to optimize costs.</li>
                    </ul>
                    <p>Systems like LinkedIn's JUDE exemplify architectures supporting multiple interaction paradigms.</p>
                </div>
                <div id="api-gateway-content" class="tab-content">
                    <h3>B. API Gateway: The Unified Entry Point</h3>
                    <p>The API Gateway acts as a single entry point for all client requests, abstracting backend complexity.</p>
                    <h4>Core Responsibilities:</h4>
                    <ul>
                        <li><strong>Request Routing:</strong> Directs requests to appropriate services or model versions.</li>
                        <li><strong>Authentication & Authorization:</strong> Enforces security policies.</li>
                        <li><strong>Rate Limiting & Throttling:</strong> Protects backend services.</li>
                        <li><strong>Request/Response Transformation:</strong> Modifies requests or responses (e.g., data format conversion).</li>
                        <li><strong>SSL/TLS Termination:</strong> Offloads encryption/decryption.</li>
                        <li><strong>Caching:</strong> Caches responses for frequently accessed, non-personalized data.</li>
                    </ul>
                    <h4>ML-Specific Considerations:</h4>
                    <ul>
                        <li><strong>Model Versioning & A/B Testing:</strong> Routes traffic to specific model versions.</li>
                        <li><strong>Protocol Translation:</strong> Bridges protocol differences (e.g., HTTP to gRPC).</li>
                    </ul>
                    <p>The API Gateway is integral to the MLOps control plane, its configuration evolving with models.</p>
                </div>
                <div id="load-balancing-content" class="tab-content">
                    <h3>C. Load Balancing Strategies</h3>
                    <p>A dedicated Load Balancer distributes requests across model serving instances to ensure HA, scalability, and optimal resource use.</p>
                    <h4>Common Strategies:</h4>
                    <ul>
                        <li><strong>Round Robin:</strong> Sequential distribution.</li>
                        <li><strong>Least Connections:</strong> To server with fewest active connections.</li>
                        <li><strong>Weighted Round Robin / Weighted Least Connections:</strong> Considers server capacity.</li>
                        <li><strong>IP Hash / Session Persistence:</strong> Routes requests from a client to the same server.</li>
                    </ul>
                    <h4>ML-Specific Challenges:</h4>
                    <ul>
                        <li><strong>Heterogeneous Workloads:</strong> Variable processing times (e.g., LLMs).</li>
                        <li><strong>GenAI-Aware Load Balancing:</strong> Newer solutions use model-specific metrics for smarter routing.</li>
                        <li><strong>Hardware Affinity:</strong> Routing to instances with specific hardware (GPUs/TPUs).</li>
                        <li><strong>Health Checks:</strong> Continuous monitoring and removal of unhealthy instances.</li>
                    </ul>
                    <p>For diverse hardware, the load balancer becomes key for active resource management.</p>
                </div>
                <div id="rate-limiting-content" class="tab-content">
                    <h3>D. Rate Limiting: Protecting System Resources</h3>
                    <p>Rate limiting controls request frequency to prevent abuse, ensure fair usage, and protect backend services.</p>
                    <h4>ML-Specific Considerations:</h4>
                    <ul>
                        <li><strong>Resource-Intensive Operations:</strong> ML inference can be computationally expensive.</li>
                        <li><strong>Cross-Model Prioritization:</strong> Advanced limiters (e.g., in Triton) allow defining resources and priorities for models.</li>
                        <li><strong>Cost Control:</strong> Prevents unexpected spikes in request volume.</li>
                        <li><strong>Granular Configuration:</strong> Based on user tiers, API keys, model endpoints.</li>
                    </ul>
                    <p>Rate limiting often functions as a multi-layered defense, from global API Gateway limits to fine-grained control at the model serving layer.</p>
                </div>
                <div id="model-serving-cluster-content" class="tab-content">
                    <h3>E. Model Serving Cluster: Kubernetes at the Helm</h3>
                    <p>The model serving cluster, typically orchestrated by Kubernetes, hosts and executes ML models.</p>
                    <h4>Kubernetes Capabilities:</h4>
                    <ul>
                        <li><strong>Scalability:</strong> Horizontal Pod Autoscaler (HPA), cluster autoscaler.</li>
                        <li><strong>Resilience & Self-Healing:</strong> Monitors and restarts/reschedules failed pods.</li>
                        <li><strong>Resource Management:</strong> Efficient allocation of CPU, memory, GPUs/TPUs.</li>
                        <li><strong>Service Discovery & Load Balancing:</strong> Built-in mechanisms.</li>
                        <li><strong>Rolling Updates & Rollbacks:</strong> Safe deployment and updates.</li>
                    </ul>
                    <h4>Deep Dive into Serving Runtimes:</h4>
                    <p>Specialized runtimes are deployed within Kubernetes pods. See the table for a comparison.</p>
                    <div class="table-responsive my-4">
                        <table class="table">
                            <thead>
                                <tr>
                                    <th>Feature</th>
                                    <th>KServe (KFServing)</th>
                                    <th>NVIDIA Triton</th>
                                    <th>Seldon Core</th>
                                    <th>Custom Runtimes</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr><td>Primary Goal</td><td>Serverless inferencing, ease of use</td><td>High-performance, multi-framework</td><td>MLOps, complex graphs</td><td>Max flexibility, LLM performance</td></tr>
                                <tr><td>Supported Frameworks</td><td>TF, PyTorch, XGBoost, sklearn, ONNX, HuggingFace</td><td>TF, PyTorch, TensorRT, ONNX, Python, OpenVINO, C++</td><td>TF, PyTorch, H2O, sklearn, custom</td><td>Any, Python-based (vLLM/SGLang)</td></tr>
                                <tr><td>Autoscaling</td><td>Yes (scale-to-zero, GPU via Knative)</td><td>Via K8s HPA</td><td>Via K8s HPA</td><td>Via K8s HPA / custom</td></tr>
                                <tr><td>Dynamic Batching</td><td>Relies on model server</td><td>Yes, configurable</td><td>Via model server / custom</td><td>Custom / library (vLLM)</td></tr>
                                <tr><td>Inference Graph/Ensemble</td><td>Yes (basic DAGs)</td><td>Yes (ensembles)</td><td>Yes (advanced graphs)</td><td>Custom</td></tr>
                                <tr><td>Model Management</td><td>Storage URI, K8s manifests</td><td>Model repository, versioning</td><td>SeldonDeployment CRD</td><td>Manual / external tools</td></tr>
                                <tr><td>Monitoring</td><td>Payload logging, drift integrations</td><td>Rich metrics (Prometheus)</td><td>Advanced metrics, logging, tracing, drift</td><td>Custom instrumentation</td></tr>
                                <tr><td>Key Strengths</td><td>Serverless, simplified deployment</td><td>GPU optimization, broad support</td><td>Comprehensive MLOps</td><td>Tailored LLM performance</td></tr>
                            </tbody>
                        </table>
                    </div>
                    <h4>Specialized Infrastructure for Large Models (e.g., DeepSeek-V3):</h4>
                    <p>Deploying models like DeepSeek-V3 (671B params) requires advanced techniques:</p>
                    <ul>
                        <li><strong>Advanced Model Architectures:</strong> Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE).</li>
                        <li><strong>Distributed Inference:</strong> Model parallelism (tensor, pipeline).</li>
                        <li><strong>Specialized Frameworks:</strong> vLLM, SGLang with Ray on Kubernetes.</li>
                        <li><strong>Hardware Requirements:</strong> Clusters of high-end GPUs (e.g., 32x A100 80GB for DeepSeek-V3).</li>
                    </ul>
                </div>
                <div id="caching-layers-content" class="tab-content">
                    <h3>F. Intelligent Caching Layers</h3>
                    <p>Caching improves performance, reduces latency, and lowers costs by reusing frequently accessed data or computation results. Common tech: Redis, Memcached.</p>
                    <h4>Types of Caching:</h4>
                    <ul>
                        <li><strong>Prediction Caching (Query Caching):</strong> Stores results of previous inference requests (e.g., LLM Prompt Caching). Effective for common queries, non-personalized results. Key design is crucial.</li>
                        <li><strong>Feature Caching:</strong> Stores pre-computed features for low-latency access, often via an online feature store. Reduces computation latency, ensures training-serving consistency.</li>
                    </ul>
                    <h4>General Caching Strategies:</h4>
                    <div class="table-responsive my-4">
                        <table class="table">
                             <thead>
                                <tr>
                                    <th>Strategy</th>
                                    <th>Description</th>
                                    <th>Pros</th>
                                    <th>Cons</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr><td>Lazy Loading</td><td>Load on cache miss</td><td>Only needed data cached</td><td>Miss penalty, stale data</td></tr>
                                <tr><td>Write-Through</td><td>Write to cache & backend simultaneously</td><td>Data always current</td><td>Higher write latency</td></tr>
                                <tr><td>Time-To-Live (TTL)</td><td>Data expires after set period</td><td>Balances freshness & hit rate</td><td>Can be stale before expiry</td></tr>
                            </tbody>
                        </table>
                    </div>
                    <p>Distributed caching enhances scalability and fault tolerance. Effective cache key design is vital for prediction caching.</p>
                </div>
                <div id="model-artifacts-content" class="tab-content">
                    <h3>G. Storing and Accessing Model Artifacts</h3>
                    <p>Model artifacts include weights, configs, tokenizers, pre/post-processing code, and metadata.</p>
                    <h4>Storage Solutions:</h4>
                    <ul>
                        <li><strong>Cloud Object Storage (S3, GCS, Azure Blob):</strong> Primary choice for large artifacts due to scalability, durability, cost. Serving runtimes load directly. For large models like DeepSeek-V3 (3TB weights), artifacts are often cached locally on PVCs.</li>
                        <li><strong>Model Registries (MLflow, Vertex AI, SageMaker):</strong> Higher-level management for versioning, staging, metadata, lineage. Often use object storage underneath.</li>
                        <li><strong>Key-Value Stores (Redis, etcd):</strong> For metadata (pointers to artifacts, version info, shard locations for large models), not large binaries.</li>
                    </ul>
                    <h4>Efficient Loading for Large Models:</h4>
                    <ul>
                        <li><strong>Model Sharding/Parallelism:</strong> Tensor and pipeline parallelism.</li>
                        <li><strong>Optimized Formats:</strong> TensorRT, ONNX.</li>
                        <li><strong>Lazy/On-demand Loading:</strong> For parts of models (e.g., MoE experts).</li>
                        <li><strong>Pre-fetching/Caching to Local PVCs.</strong></li>
                    </ul>
                    <p>A hybrid approach is common: large artifacts in object storage, metadata in a registry/KV store.</p>
                </div>
                <div id="feature-stores-content" class="tab-content">
                    <h3>H. Real-time Feature Stores</h3>
                    <p>Feature Stores bridge data and models, managing feature lifecycle for real-time predictions (e.g., recommendations, fraud detection).</p>
                    <h4>Core Components:</h4>
                    <ul>
                        <li><strong>Feature Repository (Offline Store):</strong> Data warehouse/lake for historical features (training, batch compute).</li>
                        <li><strong>Feature Transformation Engine:</strong> Defines and executes logic to create features.</li>
                        <li><strong>Feature Serving Layer (Online Store):</strong> Low-latency DB (Redis, DynamoDB) for latest feature values for inference.</li>
                        <li><strong>Metadata Store:</strong> Central catalog for feature definitions, schemas, versions, lineage.</li>
                        <li><strong>Monitoring Capabilities:</strong> For data quality, drift, freshness.</li>
                    </ul>
                    <h4>Key Functions for Inference:</h4>
                    <ul>
                        <li><strong>Low-Latency Feature Retrieval.</strong></li>
                        <li><strong>Training-Serving Skew Prevention.</strong></li>
                        <li><strong>On-demand Feature Computation.</strong></li>
                    </ul>
                    <p>Indispensable for models relying on rapidly changing data, centralizing feature logic.</p>
                </div>
                <div id="monitoring-content" class="tab-content">
                    <h3>I. Monitoring and Observability</h3>
                    <p>Essential for tracking system health, model effectiveness, detecting degradation/drift, and ensuring reliability.</p>
                    <h4>Key Metrics:</h4>
                    <ul>
                        <li><strong>System Performance:</strong> Latency (p50, p99), throughput, error rates, resource utilization (CPU, GPU).</li>
                        <li><strong>Model Performance (Prediction Quality):</strong> Accuracy, precision, recall, F1, AUC-ROC, RMSE, MAE (requires ground truth feedback).</li>
                        <li><strong>Data Drift:</strong> Statistical changes in input feature distribution.</li>
                        <li><strong>Concept Drift:</strong> Changes in feature-target relationship.</li>
                        <li><strong>Model Drift (Decay):</strong> Degradation of model performance over time.</li>
                    </ul>
                    <h4>Tools:</h4>
                    <ul>
                        <li><strong>Time-Series DB & Viz:</strong> Prometheus, Grafana.</li>
                        <li><strong>Logging Systems:</strong> ELK Stack, Loki, Splunk.</li>
                        <li><strong>Distributed Tracing:</strong> Jaeger, Zipkin.</li>
                        <li><strong>Specialized ML Monitoring:</strong> Evidently AI, Fiddler AI, Arize AI, Seldon Core.</li>
                    </ul>
                    <p>Alerting on critical thresholds is vital. ML monitoring is multi-faceted, covering system, data, and model metrics. Proactive drift detection is crucial as models can "fail silently."</p>
                </div>
            </div>
        </section>

        <section id="industry-insights" class="content-section">
            <h2>IV. Integrating Learnings from Industry Leaders</h2>
            <p>Examining architectures from industry leaders like LinkedIn's JUDE and DeepSeek-V3 provides valuable insights.</p>
            
            <h3>A. LinkedIn's JUDE: Real-time Recommendation Systems</h3>
            <p>JUDE (Job Understanding and Description Embedding) uses LLM-based representation learning for job recommendations.</p>
            <div class="table-responsive my-4">
                <table class="table">
                    <thead>
                        <tr><th>JUDE Architectural Feature</th><th>Description</th><th>Relevance to General Inference</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>Real-time Event Processing (Kafka & Samza)</td><td>Handles trillions of events daily for low-latency job matching and feature updates.</td><td>Essential for systems needing real-time responsiveness; feeds Feature Store or model inputs.</td></tr>
                        <tr><td>GPU-Accelerated LLM Inference</td><td>Serves 7B-parameter LLMs using GPUs.</td><td>Reinforces need for GPU infrastructure for LLMs.</td></tr>
                        <tr><td>Dual Online/Offline Data Sinks</td><td>Inference outputs to online Kafka (real-time recommendations) and offline Venice (analytics, retraining).</td><td>Valuable pattern: stream results for immediate use, persist for batch analytics and model improvement.</td></tr>
                    </tbody>
                </table>
            </div>
            <p>JUDE highlights the synergy between high-throughput serving and robust stream processing in dynamic, event-driven ecosystems.</p>

            <h3>B. DeepSeek-V3: Architecting for Large-Scale MoE Models</h3>
            <p>DeepSeek-V3 is a 671B parameter MoE LLM (37B active per token).</p>
            <div class="table-responsive my-4">
                <table class="table">
                    <thead>
                        <tr><th>DeepSeek-V3 Feature/Technique</th><th>Description</th><th>Relevance to Large Model Inference</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>Multi-head Latent Attention (MLA)</td><td>Reduces KV cache size for long contexts, saving memory.</td><td>KV cache optimization is vital for large LLMs.</td></tr>
                        <tr><td>DeepSeekMoE Architecture</td><td>Activates subset of "expert" parameters per token, reducing compute cost.</td><td>Scales model capacity without proportional inference cost increase; adds routing complexity.</td></tr>
                        <tr><td>FP8 Mixed-Precision</td><td>Used in training/inference, reduces memory footprint, increases speed.</td><td>Quantization is key for deploying large models.</td></tr>
                        <tr><td>Computation/Communication Overlap</td><td>Decouples computation stages to overlap communication latencies, maximizing GPU use.</td><td>Minimizing communication bottlenecks is critical for distributed inference.</td></tr>
                        <tr><td>Prefill and Decode Disaggregation</td><td>Assigns prefill and decode phases to different resource pools for optimal throughput.</td><td>Advanced scheduling for different LLM inference stages.</td></tr>
                        <tr><td>Multi-Node/Multi-GPU Parallelism</td><td>Tensor and pipeline parallelism (e.g., 4 nodes, 8 A100s each).</td><td>Essential for models too large for single device/node; needs sophisticated orchestration.</td></tr>
                        <tr><td>Specialized Serving Frameworks</td><td>Uses vLLM, SGLang, TensorRT-LLM for optimized inference.</td><td>General servers may not suffice for SOTA LLMs; specialized libraries needed.</td></tr>
                    </tbody>
                </table>
            </div>
            <p>DeepSeek-V3's deployment shows that serving SOTA LLMs is a specialized distributed systems problem, requiring deep expertise and tailored configurations beyond generic Kubernetes.</p>
        </section>

        <section id="best-practices" class="content-section">
            <h2>V. Advanced Design Considerations and Best Practices</h2>
            <p>Building a robust, scalable, and efficient ML inference system involves several advanced considerations.</p>

            <div class="space-y-4">
                <div>
                    <button class="accordion-button" data-target="bp-scalability">
                        <span>A. Achieving Scalability, Resilience, and High Availability</span>
                        <span class="accordion-icon transform rotate-0 transition-transform duration-200">&#9660;</span>
                    </button>
                    <div id="bp-scalability-content" class="accordion-content">
                        <h4>Scalability:</h4>
                        <ul>
                            <li><strong>Horizontal Pod Autoscaling (HPA):</strong> Adjusts model server pods based on metrics (CPU, GPU, custom).</li>
                            <li><strong>Cluster Autoscaler:</strong> Adds/removes Kubernetes cluster nodes.</li>
                            <li><strong>GPU Autoscaling:</strong> Specialized scaling for GPU resources (e.g., KServe via Knative).</li>
                        </ul>
                        <h4>Resilience:</h4>
                        <ul>
                            <li><strong>Redundant Components:</strong> Multiple instances of critical parts.</li>
                            <li><strong>Health Checks:</strong> Continuous monitoring by K8s and Load Balancer.</li>
                            <li><strong>Automated Recovery:</strong> K8s self-healing (restarts, reschedules).</li>
                        </ul>
                        <h4>High Availability (HA):</h4>
                        <ul>
                            <li><strong>Multi-Zone Deployments:</strong> Distribute components across availability zones.</li>
                            <li><strong>Multi-Region Deployments:</strong> For disaster recovery, with global load balancing and data replication.</li>
                        </ul>
                    </div>
                </div>

                <div>
                    <button class="accordion-button" data-target="bp-cost">
                        <span>B. Strategies for Cost-Effective Inference</span>
                        <span class="accordion-icon transform rotate-0 transition-transform duration-200">&#9660;</span>
                    </button>
                    <div id="bp-cost-content" class="accordion-content">
                        <ul>
                            <li><strong>Right-Sizing Instances:</strong> Match compute (CPU, GPU, memory) to model needs.</li>
                            <li><strong>Autoscaling to Zero:</strong> For intermittent workloads (e.g., KServe, Lambda).</li>
                            <li><strong>Maximizing GPU Utilization:</strong> Dynamic batching, concurrent model execution (Triton).</li>
                            <li><strong>Model Optimization:</strong> Quantization (FP32 to FP16/INT8/FP8), pruning, knowledge distillation.</li>
                            <li><strong>Using Spot Instances:</strong> For fault-tolerant batch or stateless real-time workloads.</li>
                        </ul>
                    </div>
                </div>

                <div>
                    <button class="accordion-button" data-target="bp-security">
                        <span>C. Security Posture for ML Inference Systems</span>
                        <span class="accordion-icon transform rotate-0 transition-transform duration-200">&#9660;</span>
                    </button>
                    <div id="bp-security-content" class="accordion-content">
                        <h4>Authentication & Authorization:</h4>
                        <ul>
                            <li><strong>API Gateway:</strong> Strong auth (API keys, OAuth2), authorization policies.</li>
                            <li><strong>Service-to-Service Auth:</strong> mTLS within service mesh.</li>
                        </ul>
                        <h4>Network Security:</h4>
                        <ul>
                            <li><strong>Firewalls & Network Policies:</strong> Least privilege access.</li>
                            <li><strong>Private Networks (VPCs):</strong> Isolate backend components.</li>
                            <li><strong>Secure Communication:</strong> TLS/SSL for external, encryption for internal.</li>
                        </ul>
                        <h4>Model Security:</h4>
                        <ul>
                            <li><strong>IP Protection:</strong> Secure storage and access controls for proprietary models.</li>
                            <li><strong>Adversarial Attack Detection/Mitigation.</strong></li>
                        </ul>
                        <h4>Data Security:</h4>
                        <ul>
                            <li><strong>Data in Transit/At Rest Encryption.</strong></li>
                            <li><strong>PII/PHI Handling:</strong> Compliance with GDPR, HIPAA (minimization, de-identification).</li>
                        </ul>
                    </div>
                </div>
                
                <div>
                    <button class="accordion-button" data-target="bp-mlops">
                        <span>D. MLOps Integration: CI/CD for Models and Infrastructure</span>
                         <span class="accordion-icon transform rotate-0 transition-transform duration-200">&#9660;</span>
                    </button>
                    <div id="bp-mlops-content" class="accordion-content">
                        <p>MLOps is essential for managing the lifecycle reliably and scalably.</p>
                        <ul>
                            <li><strong>Automated Pipelines:</strong> For model training/retraining, packaging/versioning, inference service deployment, infrastructure updates.</li>
                            <li><strong>Infrastructure as Code (IaC):</strong> Manage infrastructure (K8s, LBs, API Gateway) with code (Terraform, Helm).</li>
                            <li><strong>Model Registry as Central Hub:</strong> For versioning, metadata, promotion.</li>
                            <li><strong>Comprehensive Testing:</strong> Unit, model validation, integration, performance, data validation tests.</li>
                            <li><strong>Continuous Monitoring & Feedback Loop:</strong> Monitoring data triggers alerts, retraining, or rollbacks.</li>
                        </ul>
                        <p>MLOps unifies model development and operations into a streamlined, automated process.</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="system-architecture" class="content-section">
            <h2>III. ML Inference System Design: Diagram and Request Flows</h2>
            <p>This section presents a comprehensive system architecture diagram for a modern ML inference platform and analyzes a typical request flow. The diagram illustrates the interconnected components discussed throughout this report.</p>
            
            <h3>A. Comprehensive System Architecture Diagram</h3>
            <p>The following diagram illustrates the proposed architecture, incorporating key components like client interaction layers, the core Kubernetes-based inference platform (including model serving, caching, and data management), and supporting systems for monitoring, logging, and CI/CD.</p>
            <div class="chart-container my-6">
                <div class="mermaid">
graph TD
    %% User Interaction & Edge Layer
    subgraph User_Interaction_Edge ["User Interaction & Edge Layer"]
        direction LR
        U_Client["Clients (Web/Mobile/Services)"]
        U_APIGateway["API Gateway (AuthN/Z, Routing, Req/Resp Transform, A/B Test Control, Basic Cache)"]
        U_RateLimiter["Rate Limiter (Quota Mgmt)"]
        U_LoadBalancer["L7 Load Balancer (SSL Term, Health Checks, Adv. Routing)"]
        
        U_Client --> U_APIGateway
        U_APIGateway --> U_RateLimiter
        U_RateLimiter --> U_LoadBalancer
    end

    %% Core Inference Platform (Kubernetes)
    subgraph Core_Inference_Platform_K8s ["Core Inference Platform (Kubernetes Cluster)"]
        direction TB
        
        subgraph K8s_Control_Plane_Group ["Kubernetes Control Plane"]
            K8s_API["K8s API Server"]
            K8s_Scheduler["K8s Scheduler"]
            K8s_ControllerMgr["K8s Controller Manager"]
            K8s_HPA["Horizontal Pod Autoscaler (HPA)"]
            K8s_ClusterAutoscaler["Cluster Autoscaler"]
            
            K8s_HPA -.->|Adjusts Pod Replicas| K8s_API
            K8s_ClusterAutoscaler -.->|Adjusts Node Count| K8s_API
        end

        subgraph K8s_Worker_Nodes ["Kubernetes Worker Nodes (with GPU/CPU Resources)"]
            direction LR
            subgraph Model_Serving_Pods ["Model Serving Pods (Replicas)"]
                MSP_A1["Pod 1: Serving Runtime (e.g., Triton/KServe/Custom FastAPI+vLLM) <br/> GPU/CPU Resource <br/> Model Shard/Instance"]
                MSP_A2["Pod 2: Serving Runtime <br/> GPU/CPU Resource <br/> Model Shard/Instance"]
                MSP_An["Pod N: Serving Runtime <br/> GPU/CPU Resource <br/> Model Shard/Instance"]
            end
            K8s_PVCs["Persistent Volume Claims (PVCs for local model/artifact cache)"]
            
            MSP_A1 --- K8s_PVCs
            MSP_A2 --- K8s_PVCs
            MSP_An --- K8s_PVCs
        end
        
        K8s_API -->|Manages| K8s_Worker_Nodes
        U_LoadBalancer -->|Distributes Traffic via K8s Service| Model_Serving_Pods

        subgraph Caching_State ["Caching & State (Distributed)"]
            direction LR
            CS_PredCache["Prediction Cache (e.g., Redis Cluster)"]
            CS_FeatStoreOnline["Feature Store - Online (e.g., Redis/DynamoDB)"]
        end
        Model_Serving_Pods -.->|R/W Predictions| CS_PredCache
        Model_Serving_Pods -.->|Read Features| CS_FeatStoreOnline

        subgraph Model_Data_Management_Services ["Model & Data Management Services"]
            direction TB
            MD_ModelRegistry["Model Registry (Versioning, Metadata, Staging)"]
            MD_ArtifactStore["Model Artifact Store (e.g., S3/GCS - Weights, Configs, Tokenizers)"]
            MD_FeatStoreOffline["Feature Store - Offline (e.g., Data Warehouse/Lake)"]
            MD_KVModelMeta["KV Store (Model Shard Info, Dynamic Configs)"]
            MD_DataPipelines["Data Pipelines (ETL/ELT for Feature Store Offline)"]
            
            MD_DataPipelines --> MD_FeatStoreOffline
        end
        
        MD_ModelRegistry -.->|Provides Model Info| Model_Serving_Pods
        MD_ArtifactStore -.->|Source for PVCs/Direct Load| Model_Serving_Pods
        MD_KVModelMeta -.->|Provides Shard/Config Info| Model_Serving_Pods
        MD_FeatStoreOffline -.->|Syncs Features| CS_FeatStoreOnline
    end

    %% Supporting Systems & MLOps
    subgraph Supporting_Systems_MLOps ["Supporting Systems & MLOps"]
        direction TB
        SS_Monitoring["Monitoring & Alerting (e.g., Prometheus, Grafana, Alertmanager)"]
        SS_Logging["Centralized Logging (e.g., ELK Stack, Loki)"]
        SS_Tracing["Distributed Tracing (e.g., Jaeger, OpenTelemetry)"]
        SS_CICD_Pipeline["CI/CD Pipeline (e.g., Jenkins, GitLab CI, ArgoCD)"]
        SS_StreamProcessor["Stream Processor (e.g., Kafka, Flink/Samza for real-time features/events)"]
        SS_FeedbackCollector["Feedback Collector (Ground Truth for Model Perf. Monitoring)"]

        Model_Serving_Pods -.->|Metrics, Logs, Traces| SS_Monitoring
        Model_Serving_Pods -.->|Logs| SS_Logging
        Model_Serving_Pods -.->|Traces| SS_Tracing
        U_APIGateway -.->|Metrics, Logs| SS_Monitoring
        U_APIGateway -.->|Logs| SS_Logging
        
        SS_CICD_Pipeline -->|Deploy Model Config| MD_ModelRegistry
        SS_CICD_Pipeline -->|Push Artifacts| MD_ArtifactStore
        SS_CICD_Pipeline -->|Deploy/Update K8s Manifests| K8s_API
        SS_CICD_Pipeline -->|Update API Gateway Rules| U_APIGateway
        
        SS_StreamProcessor -->|Real-time Features/Events| CS_FeatStoreOnline
        SS_StreamProcessor -->|"Real-time Events (e.g., JUDE-like)"| Model_Serving_Pods
        
        SS_FeedbackCollector -.->|Ground Truth| SS_Monitoring
        SS_Monitoring -.->|Alerts/Triggers Retraining| SS_CICD_Pipeline
    end

    %% Primary Request/Response Flow (Simplified labels for clarity on diagram)
    U_Client -.->|1. Req| U_APIGateway
    U_APIGateway -.->|2. Auth/Route| U_RateLimiter
    U_RateLimiter -.->|3. Limit Check| U_LoadBalancer
    U_LoadBalancer -.->|4. To Pod| MSP_A1
    MSP_A1 -.->|5a. Cache?| CS_PredCache
    CS_PredCache -.->|5b. Hit/Miss| MSP_A1
    MSP_A1 -.->|6a. Features?| CS_FeatStoreOnline
    CS_FeatStoreOnline -.->|6b. Features| MSP_A1
    MSP_A1 -.->|"Inference"| MSP_A1 
    MSP_A1 -.->|7. Resp| U_LoadBalancer 
    U_LoadBalancer -.-> U_RateLimiter
    U_RateLimiter -.-> U_APIGateway
    U_APIGateway -.->|8. Final Resp| U_Client

    classDef userEdge fill:#E0F2FE,stroke:#7DD3FC,stroke-width:2px,color:#0C4A6E;
    classDef k8sCore fill:#FEFCE8,stroke:#FACC15,stroke-width:2px,color:#713F12;
    classDef k8sCtrl fill:#EFF6FF,stroke:#93C5FD,stroke-width:1px,color:#1E3A8A;
    classDef k8sWorker fill:#F0FDF4,stroke:#86EFAC,stroke-width:1px,color:#15803D;
    classDef caching fill:#FFFBEB,stroke:#FDE047,stroke-width:1px,color:#713F12;
    classDef dataMgmt fill:#F0FDFA,stroke:#67E8F9,stroke-width:1px,color:#0E7490;
    classDef supportMLOps fill:#FCE7F3,stroke:#F9A8D4,stroke-width:2px,color:#831843;

    class U_Client,U_APIGateway,U_RateLimiter,U_LoadBalancer userEdge;
    class K8s_API,K8s_Scheduler,K8s_ControllerMgr,K8s_HPA,K8s_ClusterAutoscaler k8sCtrl;
    class MSP_A1,MSP_A2,MSP_An,K8s_PVCs k8sWorker;
    class CS_PredCache,CS_FeatStoreOnline caching;
    class MD_ModelRegistry,MD_ArtifactStore,MD_FeatStoreOffline,MD_KVModelMeta,MD_DataPipelines dataMgmt;
    class SS_Monitoring,SS_Logging,SS_Tracing,SS_CICD_Pipeline,SS_StreamProcessor,SS_FeedbackCollector supportMLOps;
                </div>
            </div>

            <h3>B. Detailed Explanation of Diagram Components and Interactions</h3>
            <p>The architecture is organized into logical groups:</p>
            <ul>
                <li><strong>User Interaction & Edge:</strong> Client applications, API Gateway, Rate Limiter, Load Balancer.</li>
                <li><strong>Core Inference Platform (Kubernetes):</strong>
                    <ul>
                        <li><strong>Model Serving Cluster:</strong> K8s Control Plane, Model Server Pods (Triton, KServe, etc.).</li>
                        <li><strong>Caching & State:</strong> Prediction Cache (Redis), Feature Store Online Cache (Redis).</li>
                        <li><strong>Model & Data Management:</strong> Model Registry, Model Artifact Store (S3), Feature Store Offline, KV Store for model metadata.</li>
                    </ul>
                </li>
                <li><strong>Supporting Systems:</strong> Monitoring (Prometheus), Logging (ELK), CI/CD Pipeline, Stream Processor (Kafka).</li>
            </ul>
            
            <h3>C. End-to-End Request Flow Analysis (Real-time Synchronous)</h3>
            <p>A typical prediction request flows as follows:</p>
            <ol class="list-decimal list-inside space-y-2">
                <li><strong>Client Request:</strong> Client sends request to API Gateway.</li>
                <li><strong>Gateway Processing:</strong> API Gateway handles AuthN/AuthZ, passes to Rate Limiter. Rate Limiter checks quota.</li>
                <li><strong>Load Balancing:</strong> Request forwarded to Load Balancer, which selects an available Model Server Pod.</li>
                <li><strong>Model Server Pod Processing:</strong>
                    <ul class="list-disc list-inside ml-4 space-y-1">
                        <li>(Optional) Checks Prediction Cache. If hit, returns cached prediction.</li>
                        <li>(Optional) If cache miss, queries Feature Store (Online) for features.</li>
                        <li>Model (already loaded from Model Artifact Store via Model Registry/KV Store, potentially cached on PVC) performs inference.</li>
                        <li>(Optional) Updates Prediction Cache with new result.</li>
                    </ul>
                </li>
                <li><strong>Response Path:</strong> Prediction returns via Load Balancer, Rate Limiter to API Gateway.</li>
                <li><strong>Final Response to Client:</strong> API Gateway sends formatted prediction to Client.</li>
                <li><strong>Parallel Monitoring/Logging:</strong> Components send logs and metrics to Logging/Monitoring Systems. CI/CD pipelines manage deployments and updates. Stream processors may feed real-time data.</li>
            </ol>
            <p>This flow includes crucial side-paths for caching and feedback loops for monitoring, which can trigger alerts or retraining.</p>
        </section>

        <section id="future-trends" class="content-section">
            <h2>VI. Conclusion and Future Outlook</h2>
            <p>Designing high-performance ML inference systems is a complex engineering challenge. The architecture presented provides a blueprint based on modularity, scalability, performance optimization, data-centricity, observability, and MLOps automation.</p>
            
            <h3>Recap of Key Design Tenets:</h3>
            <ul>
                <li>Modularity and Abstraction (API Gateways, Load Balancers, Serving Runtimes).</li>
                <li>Scalability and Resilience (Kubernetes, Autoscaling).</li>
                <li>Performance Optimization (Batching, Quantization, Caching).</li>
                <li>Data-Centricity (Model Artifact Management, Feature Stores).</li>
                <li>Observability (Comprehensive Monitoring).</li>
                <li>Automation through MLOps (CI/CD).</li>
            </ul>
            <p>Insights from JUDE (real-time event processing, dual data sinks) and DeepSeek-V3 (specialized techniques for LLMs) further refine these tenets.</p>

            <h3>Emerging Trends and Future Directions:</h3>
            <ul>
                <li><strong>Hardware Specialization and Co-design:</strong> More efficient AI accelerators, hardware-software co-design.</li>
                <li><strong>Advancements in LLM Serving:</strong> Better KV cache management, speculative decoding, MoE routing.</li>
                <li><strong>Serverless Inference Proliferation:</strong> Cost-efficiency for intermittent workloads.</li>
                <li><strong>GenAI-Aware Infrastructure:</strong> LBs, orchestrators, monitoring tools tailored for GenAI.</li>
                <li><strong>Standardization and Maturity of MLOps:</strong> Easier end-to-end lifecycle management.</li>
                <li><strong>Responsible AI in Production:</strong> Built-in explainability, fairness, bias detection.</li>
                <li><strong>Edge and Hybrid Deployments:</strong> For low-latency IoT and autonomous systems.</li>
            </ul>
            <p>Effective ML inference systems require a deep understanding of software engineering and ML demands, adapting to the continuous evolution of AI.</p>
        </section>
    </main>

    <footer class="bg-stone-800 text-stone-300 py-8 text-center">
        <p>&copy; 2024 Interactive Report on ML Inference System Design. Information based on the provided report.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const navLinks = document.querySelectorAll('header nav a.nav-link, #mobile-menu a.nav-link');
            const sections = document.querySelectorAll('.content-section');
            const mobileMenuButton = document.getElementById('mobile-menu-button');
            const mobileMenu = document.getElementById('mobile-menu');
            
            function updateActiveSection() {
                let currentSectionId = window.location.hash || '#introduction';
                if (currentSectionId === '#') currentSectionId = '#introduction';

                sections.forEach(section => {
                    if ('#' + section.id === currentSectionId) {
                        section.classList.add('active');
                    } else {
                        section.classList.remove('active');
                    }
                });

                navLinks.forEach(link => {
                    if (link.getAttribute('href') === currentSectionId) {
                        link.classList.add('active');
                    } else {
                        link.classList.remove('active');
                    }
                });
                document.querySelectorAll('#mobile-menu a.nav-link').forEach(link => {
                     if (link.getAttribute('href') === currentSectionId) {
                        link.classList.add('active');
                    } else {
                        link.classList.remove('active');
                    }
                });
            }

            navLinks.forEach(link => {
                link.addEventListener('click', (e) => {
                    if (mobileMenu.classList.contains('block')) {
                         mobileMenu.classList.remove('block');
                         mobileMenu.classList.add('hidden');
                         mobileMenuButton.querySelectorAll('svg')[0].classList.remove('hidden');
                         mobileMenuButton.querySelectorAll('svg')[1].classList.add('hidden');
                    }
                });
            });
            
            window.addEventListener('hashchange', updateActiveSection);
            updateActiveSection(); 

            mobileMenuButton.addEventListener('click', () => {
                const isMenuOpen = mobileMenu.classList.toggle('hidden');
                mobileMenu.classList.toggle('block', !isMenuOpen);
                mobileMenuButton.querySelectorAll('svg').forEach(svg => svg.classList.toggle('hidden'));
            });

            const coreComponentTabs = document.querySelectorAll('#core-components-tabs .tab-button');
            const coreComponentTabContents = document.querySelectorAll('#core-components .tab-content');

            coreComponentTabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    coreComponentTabs.forEach(t => t.classList.remove('active'));
                    tab.classList.add('active');

                    const targetContentId = tab.dataset.target + '-content';
                    coreComponentTabContents.forEach(content => {
                        if (content.id === targetContentId) {
                            content.classList.add('active');
                        } else {
                            content.classList.remove('active');
                        }
                    });
                });
            });
            
            const accordionButtons = document.querySelectorAll('.accordion-button');
            accordionButtons.forEach(button => {
                button.addEventListener('click', () => {
                    const contentId = button.dataset.target + '-content';
                    const content = document.getElementById(contentId);
                    const icon = button.querySelector('.accordion-icon');

                    const isCurrentlyHidden = content.classList.contains('hidden');
                    content.classList.toggle('hidden', !isCurrentlyHidden); 
                    icon.classList.toggle('rotate-180', isCurrentlyHidden); 
                });
            });

        });
    </script>
</body>
</html>
